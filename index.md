## Visual Generative Modeling Workshop 2022

### Empowering Visual Data with Generative Models: From Research to Production

**Please note: this is a NeurIPS workshop proposal. Some information on this page might be revised and updated upon workshop acceptance.**

Welcome to the first Visual Generative Modeling workshop (VGM 2022)! 

Photorealistic image synthesis has achieved fantastic progress in recent years. In additional to image synthesis by physics-based simulations, recent advances introduce a new family of generative image modeling techniques with notable image realism by using generative adversarial networks (GANs) and neural radiance fields (NeRFs). 
This creates new opportunities to use generative models for data synthesis, thereby generating realistic data in a large scale in production. 
These data can be used in practice to train neural networks, e.g., enhancing perception models in autonomous driving, or generating digital human in computer animation.
Despite such, there remains open challenges to successfully adopting generative models for data synthesis such as data realism, the domain gap between synthetic and real data, or generating labels together with data, etc.

VGM 2022 aims to discuss about generative modeling and data synthesis in both research and production, bringing together researchers, practitioners, engineers, students, and enthusiasts to discuss about the past, present, and future of generative modeling and data synthesis. 
The workshop includes but is not limited to the discussion of the following topics: 

- Visual data production using simulation techniques and the translation between simulation and real scenarios, 

- Generative modeling techniques including GANs and NeRFs for images, videos, and related domains such as medical images,

- Image synthesis techniques in real scenarios including but not limited to dataset creation, data annotation, user interaction, and data augmentation.  

- Application of data generation in related industries such as autonomous driving, computer animation, and virtual/augmented/mixed reality.

The workshop is a full-day **virtual** event. 
All talks are pre-recorded. The poster and panel discussion will be livestreamed on StreamYard and YouTube. 


### Workshop Paper Submission 

We invite contributors to submit full papers (max 9 pages excluding references) and short papers (max 5 pages excluding references). 
All submissions must be in PDF using the [NeurIPS 2022 template](), and written in English. 
Short papers include work-in-progress and demo papers. Following NeurIPS 2022 workshop guidelines, we would only accept papers with original content. Papers created for presenting a work already published in another venue will not be accepted. 

The submissions and reviews are double-blind. All reviewer comments on all accepted papers will be made publicly available. Each paper will be reviewed by at least three reviewers. 
For each accepted paper, at least an author must attend the workshop to present the work. 

Please use [OpenReview]() to start your submission! 


### Important Dates 

TBD 

### Invited Speakers

TBD 

### Program

TBD

### Organizers 

- [Anh Tran](), VinAI

- [Binh-Son Hua](), VinAI

- [Khoi Nguyen](), VinAI

- [Dinh Phung](), Monash University

- [Jianfei Cai](), Monash University

